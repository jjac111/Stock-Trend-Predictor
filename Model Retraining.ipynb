{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "The modeling phase of the machine learning workflow consists in defining the models to be trained, train the created models, and subsequently test each model's accuracy.\n",
    "\n",
    "I am using Amazon Sagemaker for the training, testing, and deployment of the defined models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from statistics import mean, mode, median, stdev\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to use for this notebook\n",
    "\n",
    "\n",
    "time_words = {1:'day', 5:'week', 10:'two_weeks', 20:'month', 90:'four_months', 270:'year'}\n",
    "\n",
    "\n",
    "# directory containing training and testing datasets\n",
    "data_dir = join('data_1/') \n",
    "final_data_dir = join(data_dir + 'final/')\n",
    "top_results_file = 'results/top_accuracy.txt'\n",
    "model_directory = 'models_1/'\n",
    "\n",
    "with open(top_results_file) as f:\n",
    "    text = f.read()\n",
    "    top_5 = [line for line in text.split('\\n')]\n",
    "    top_5 = [(p.split()[0], p.split()[1]) for p in top_5]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "Under the ``source/`` directory is the file ``model.py``, which contains the definition for a class named ``BinaryClassifier``. This class defines our base ANN model for this project which hast the following structure:\n",
    "1. Three parameters need to be passed to the model:\n",
    "    * ``input_features``: the number of neurons to create for input (11 in this case)\n",
    "    * ``hidden_dim``: a parameter used to define the ANN hidden layers.\n",
    "    * ``output_dim``: the number of neurons in the final layer of the ANN. For a binary classifier this is 1, and the result ranges from [0,1].\n",
    "2. The number of neurons in the 4 hidden layers of the model are defined as:\n",
    "    * ``hidden_dim``\n",
    "    * ``2 * hidden_dim``\n",
    "    * ``3 * hidden_dim``\n",
    "    * ``hidden_dim``\n",
    "3. The forward pass of the model\n",
    "    * Input layer -> Linear transform to the first hidden layer\n",
    "    * Passed into Rectifier Linear Unit function\n",
    "    * Dropout layer (for training only)\n",
    "    * Repeat the above steps until the final hidden layer...\n",
    "    * Last hidden layer -> Linear transform to the output layer\n",
    "    * Sigmoid Activation Function -> Result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define this model's hidden layer nodes and parameters\n",
    "input_dim = 11\n",
    "d1 = 500\n",
    "d2 = 2*d1\n",
    "d3 = 3*d1\n",
    "d4 = d1\n",
    "activation = 'relu'\n",
    "dropout = 0.2\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"270-90-10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 500)               6000      \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 1500)              1501500   \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 500)               750500    \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 1)                 501       \n",
      "_________________________________________________________________\n",
      "sigmoid (Activation)         (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,759,501\n",
      "Trainable params: 2,759,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for m, _ in top_5:\n",
    "    input_layer = Input(name='the_input', shape=(input_dim,), batch_shape=(None, input_dim))\n",
    "    # Add dense layers\n",
    "    dense_1 = Dense(d1, activation=activation)(input_layer)\n",
    "    drop_1 = Dropout(dropout)(dense_1)\n",
    "    dense_2 = Dense(d2, activation=activation)(drop_1)\n",
    "    drop_2 = Dropout(dropout)(dense_2)\n",
    "    dense_3 = Dense(d3, activation=activation)(drop_2)\n",
    "    drop_3 = Dropout(dropout)(dense_3)\n",
    "    dense_4 = Dense(d4, activation=activation)(drop_3)\n",
    "    drop_4 = Dropout(dropout)(dense_4)\n",
    "    dense_5 = Dense(1, activation=activation)(drop_4)\n",
    "\n",
    "    # Add sigmoid activation layer\n",
    "    y_pred = Activation('sigmoid', name='sigmoid')(dense_5)\n",
    "    \n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_layer, outputs=y_pred, name=m)\n",
    "    model.output_length = lambda x: x\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    models.append(model)\n",
    "    \n",
    "print(models[0].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "The model training will be performed by Amazon Sagemaker. Training jobs will be created for each training dataset in the ``final/`` directory. Under ``source/`` there is a file named ``train.py``, which contains the structure for a PyTorch entry point. This is necesssary for creating estimators through Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and testing data\n",
    "data = {}\n",
    "for mod, _ in top_5:\n",
    "    data[mod] = {}\n",
    "    m = mod.split('-')\n",
    "    train_file = final_data_dir+time_words[int(m[0])]+'/'+m[1]+'_'+m[2]+'/train.csv'\n",
    "    test_file = final_data_dir+time_words[int(m[0])]+'/'+m[1]+'_'+m[2]+'/test.csv'\n",
    "\n",
    "    df_train = pd.read_csv(train_file, header=None)\n",
    "    df_test = pd.read_csv(test_file, header=None)\n",
    "\n",
    "    y_train = df_train[0]\n",
    "    X_train = df_train.drop(labels=0, axis=1)\n",
    "    y_test = df_test[0]\n",
    "    X_test = df_test.drop(labels=0, axis=1)\n",
    "\n",
    "    data[mod]['y_train'] = y_train\n",
    "    data[mod]['X_train'] = X_train\n",
    "    data[mod]['y_test'] = y_test\n",
    "    data[mod]['X_test'] = X_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6930 - accuracy: 0.4037\n",
      "Epoch 2/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 3/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 4/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 5/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 6/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 7/10\n",
      "18096/18096 [==============================] - 46s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 8/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 9/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 10/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Finished training model: 270-90-10\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6932 - accuracy: 0.4005\n",
      "Epoch 2/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 3/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 4/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 5/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 6/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 7/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 8/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006 0s - loss: 0.6931 \n",
      "Epoch 9/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 10/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Finished training model: 270-90-270\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6932 - accuracy: 0.4006\n",
      "Epoch 2/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 3/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 4/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 5/10\n",
      "18096/18096 [==============================] - 43s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 6/10\n",
      "18096/18096 [==============================] - 43s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 7/10\n",
      "18096/18096 [==============================] - 43s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 8/10\n",
      "18096/18096 [==============================] - 43s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 9/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 10/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Finished training model: 90-270-10\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6932 - accuracy: 0.4006\n",
      "Epoch 2/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 3/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 4/10\n",
      "18096/18096 [==============================] - 44s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 5/10\n",
      "18096/18096 [==============================] - 45s 2ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 6/10\n",
      "18096/18096 [==============================] - 60s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 7/10\n",
      "18096/18096 [==============================] - 63s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 8/10\n",
      "18096/18096 [==============================] - 64s 4ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 9/10\n",
      "18096/18096 [==============================] - 64s 4ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 10/10\n",
      "18096/18096 [==============================] - 56s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Finished training model: 90-270-90\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "18096/18096 [==============================] - 47s 3ms/step - loss: 0.6933 - accuracy: 0.4030\n",
      "Epoch 2/10\n",
      "18096/18096 [==============================] - 47s 3ms/step - loss: 0.6931 - accuracy: 0.4006 \n",
      "Epoch 3/10\n",
      "18096/18096 [==============================] - 46s 3ms/step - loss: 0.6931 - accuracy: 0.4006 0s - l\n",
      "Epoch 4/10\n",
      "18096/18096 [==============================] - 47s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 5/10\n",
      "18096/18096 [==============================] - 46s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 6/10\n",
      "18096/18096 [==============================] - 46s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 7/10\n",
      "18096/18096 [==============================] - 47s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 8/10\n",
      "18096/18096 [==============================] - 46s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 9/10\n",
      "18096/18096 [==============================] - 47s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Epoch 10/10\n",
      "18096/18096 [==============================] - 46s 3ms/step - loss: 0.6931 - accuracy: 0.4006\n",
      "Finished training model: 20-10-270\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    name = model.name\n",
    "    model.fit(x=X_train, \n",
    "              y=y_train,\n",
    "              batch_size=batch,\n",
    "              epochs=epochs)\n",
    "    \n",
    "    model.save(model_directory+name+'.h5')\n",
    "    print('Finished training model: '+name+'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "For evaluation I am deploying each training job created. For each estimator, a predictor endpoint is created briefly to be sent data to make predictions. For each predictor, their respective test datasets are passed. The endpoint for the predictor is then deleted. Then, accuracy calculations are made against the labeled test datasets, they are printed and stored into ``.txt`` files under the ``results/`` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "10530/10530 [==============================] - 2s 186us/step\n",
      "[0.6931471824645996, 0.05593542382121086]\n",
      "['loss', 'accuracy']\n",
      "5811/5811 [==============================] - 1s 191us/step\n",
      "[0.6931471824645996, 0.4661848247051239]\n",
      "['loss', 'accuracy']\n",
      "8190/8190 [==============================] - 2s 195us/step\n",
      "[0.6931471822462676, 0.38840049505233765]\n",
      "['loss', 'accuracy']\n",
      "8190/8190 [==============================] - 1s 182us/step\n",
      "[0.6931471822462676, 0.38840049505233765]\n",
      "['loss', 'accuracy']\n",
      "9048/9048 [==============================] - 2s 182us/step\n",
      "[0.6931471824645996, 0.40163570642471313]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate(data[model.name]['X_test'], data[model.name]['y_test'], verbose=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
